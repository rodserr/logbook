{
  "hash": "ca0a604c9f3cdbb45f4a1742764357e1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tutorial: Replicating data to BigQuery\"\ndescription: |\n  Quick Tutorial on how to automate data replication to BigQuery using Google Cloud Functions in order to build BI/ML tools\nauthor:\n  - name: Rodrigo Serrano\n    url: https://rodserr.github.io/logbook\ndate: 2022-02-10\n---\n\n\n\n\nBigQuery can be used as Data Warehouse to gather multiple silos and build BI tools or ML models. This tutorial is very general, for details check Google documentation. I'm recording my steps using Hubspot as the source, overall it would be something like:\n\n1- Write a function that extracts data from Hubspot (or any other source that can be pragmatically scheduled), and load it to BigQuery\n\n2- Upload the script to [Google Cloud Functions](https://cloud.google.com/functions/docs/quickstart-python) and set a routine with [Cloud Scheduler](https://cloud.google.com/scheduler/docs)\n\n3- Handle duplicates \n\n> Cloud Functions doesn't support R runtime, there is a way to do the same routine with R using {googleCloudRunner} but I found it harder to implement than use Functions\n\n## Create a Service Account\n\nFirst, you will have to create a project and set the [service account](https://cloud.google.com/iam/docs/service-accounts) permission to the resources, at least these three: BigQuery, Functions, and Scheduler. For local development, you will need to download the Key as JSON file and place it in your project root\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom google.cloud import bigquery\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/.secrets/project_id.json\"\nbqclient = bigquery.Client()\n```\n:::\n\n\n## Plan the schema and relations\n\nSketch the relation between tables, datasets and be sure about data types. I usually define a dataset for every silo (data source)\n\n## Set the Pipeline\n\nAfter replicating the first batch of data we need to define how we will update the tables. Basically, every X minutes, hours, or days, depending on the frequency of the update, take the datetime of the last updated (or created) record saved in BQ, and query records created/updated in the source after that timestamp.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_last_updated_object(bq_client, table_name, field_name):\n  \n  \"\"\"\n  bq_client: BigQuery Client\n  table_name: str table to query\n  field_name: str name of the date/datetime field to get max value\n  \n  return: max field_name in table_name in unix\n  \"\"\"\n  \n  checkpoint_query = f\"SELECT max({field_name}) as last_updated FROM `poject_name.hubspot.{table_name}` \"\n  checkpoints_df = bq_client.query(checkpoint_query, project='project_name').to_dataframe()\n  \n  last_updated_datetime = checkpoints_df['last_updated'][0]\n  \n  last_updated_unix = dt.datetime.timestamp(last_updated_datetime)\n  \n  return last_updated_unix.__round__()\n```\n:::\n\n\n## Compressing an ETL task into a single function\n\nThe idea is to extract the data, transform into dataframes, and append it to the corresponding BQ table. We will query HubSpot's deals as an example\n\n### Extract\n\nHubspot API uses a pagination system for querying, so all this function does is retrieve data from a timestamp. The timestamp serves as a checkpoint, we will load only the batch of data after that checkpoint.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_recent_modified(url, hapikey, count, since, max_results):\n  \n  \"\"\"\n  url: str endpoint to retreive. One of deals, companies or engagements\n  hapikey: str API key\n  count: dbl number of object to retreive in a single call\n  since: dbl unix datetime in miliseconds to retreive after\n  max_results: dbl max number of records to retreive\n  \n  return: list with response\n  \"\"\"\n  \n  object_list = []\n  get_recent_url = url\n  parameter_dict = {'hapikey': hapikey, 'count': count, 'since': since}\n  headers = {}\n  \n  # Paginate your request using offset\n  has_more = True\n  while has_more:\n    parameters = urllib.parse.urlencode(parameter_dict)\n    get_url = get_recent_url + parameters\n    r = requests.get(url= get_url, headers = headers)\n    response_dict = json.loads(r.text)\n    has_more = response_dict['hasMore']\n    object_list.extend(response_dict['results'])\n    parameter_dict['offset'] = response_dict['offset']\n    \n    if len(object_list) >= max_results: # Exit pagination, based on whatever value you've set your max results variable to.\n      print('maximum number of results exceded')\n      break\n  \n  print(f'Done!! Found {len(object_list)} object')\n  \n  return object_list\n```\n:::\n\n\n### Transform\n\nDo the necessary wrangling to convert the data to a ready-to-load pd.DataFrame, and make sure about column types. In this case, I extract some association id attributes from the list response and some attributes that are in another field of the list response, I merged and convert them to the correct field types.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef extract_value(new_objects, column_names):\n  \n  \"\"\"\n  new_objects: list with http response\n  column_names: list with column names to keep\n  \n  return: pd.DataFrame with column_names fields\n  \"\"\"\n  \n  has_associations = 'associations' in new_objects[0].keys()\n  \n  list_properties = []\n  for obj in new_objects:\n    \n    if has_associations:\n      associatedCompanyIds = obj['associations']['associatedCompanyIds']\n      associatedVids = obj['associations']['associatedVids']\n    \n    props = obj['properties']\n    \n    saved_properties = {}\n    for key, value in props.items():\n      saved_properties[key] = value['value']\n    \n    if has_associations:\n      saved_properties['associatedCompanyIds'] = associatedCompanyIds\n      saved_properties['associatedVids'] = associatedVids\n      \n    list_properties.append(saved_properties)\n  \n  df_properties = pd.DataFrame(list_properties)\n  \n  subset_columns = df_properties.columns.intersection(set(column_names))\n  \n  df_properties = df_properties[subset_columns]\n  \n  if has_associations:\n    df_properties['associatedVids'] = df_properties['associatedVids'].apply(lambda x: '-'.join(map(str, x)))\n    df_properties = df_properties.explode('associatedCompanyIds')\n    \n  return df_properties\n\n\n\ndef parse_properties(df, columns_to_datetime, columns_to_numeric, columns_to_boolean):\n  \n  \"\"\"\n  df: pd.DataFrame\n  columns_to_datetime, columns_to_numeric, columns_to_boolean: list with names of the columns to parse\n  \n  return: pd.DataFrame with parsed columns\n  \"\"\"\n  \n  if columns_to_datetime:\n    df[columns_to_datetime] = df[columns_to_datetime].apply(pd.to_datetime, unit =  'ms')\n  \n  if columns_to_numeric:\n    df[columns_to_numeric] = df[columns_to_numeric].apply(pd.to_numeric, errors = 'coerce')\n    \n  if columns_to_boolean:\n    df[columns_to_boolean] = df[columns_to_boolean].replace({'true': True, 'false': False}).astype('boolean')\n  \n  return df\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef clean_deals(deals_list):\n  \n  \"\"\"\n  deals_list: list with http response\n  \n  return: pd.DataFrame with necessary columns and correct types\n  \"\"\"\n  \n  deal_properties = ['amount_in_home_currency', 'closed_lost_reason', 'closedate', 'createdate', 'days_to_close', 'deal_currency_code', \n                     'dealname', 'dealstage', 'dealtype', 'hs_is_closed', 'hs_is_closed_won', 'hs_lastmodifieddate', 'hs_object_id', \n                     'associatedCompanyIds', 'associatedVids', 'pipeline']\n  \n  deals = extract_value(deals_list, deal_properties)\n  deals = parse_properties(\n    deals,\n    columns_to_datetime = ['createdate', 'closedate', 'hs_lastmodifieddate'],\n    columns_to_numeric = ['days_to_close', 'hs_object_id', 'amount_in_home_currency'],\n    columns_to_boolean = ['hs_is_closed', 'hs_is_closed_won']\n    )\n    \n  return deals\n```\n:::\n\n\n### Load\n\nUse the BigQuery API to `load_table_from_dataframe()`, previously define the schema to avoid data type errors between the upload batch and the BQ table.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Example Schema\nexample_schema = [\n  bigquery.SchemaField(\"amount_in_home_currency\", \"FLOAT\"),\n  bigquery.SchemaField(\"closed_lost_reason\", \"STRING\"),\n  bigquery.SchemaField(\"closedate\", \"DATE\"),\n  bigquery.SchemaField(\"days_to_close\", \"INTEGER\"))]\n  \n  \ndef load_table_from_dataframe_safely(bq_client, df: pd.DataFrame, table_id: str, table_schema: list):\n    \n    \"\"\"\n    df: dataframe to upload to BigQuery\n    table_id: id of table in BigQuery, in should consist of project.dataset.table\n    table_schema: list of .SchemaField element for every column in the table\n    \n    return: nothing, it uploads the df to BQ in the table_id destination\n    \"\"\"\n    \n    if df.empty:\n        return(print(f'Empty Table, there are not new records in {table_id}'))\n    else:\n        job_config = bigquery.LoadJobConfig(schema=table_schema)\n        job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_config)\n        return(job.result())\n```\n:::\n\n\n### Include all into a single function\n\nCloud function executes a function, not a script, so all the helpers functions need to be used in one function.\nThere are three places where helpers can live:\n\n* In the same script as the main function (you will define what is the function that GCP will execute, so it doesn't matter to have variables and helpers in the same script)\n\n* In a different script that lives in a folder at the same level directory that main.py\n\n* Build a library and add it to dependencies\n\nI think 3rd one is the best choice but adds complexity to the process, so we will go with the 2nd one to be able to use the helpers in local development\n\nThe final main.py will be like\n\n\n::: {.cell}\n\n```{.python .cell-code}\n########################################## Libraries\nimport pandas as pd\nimport datetime as dt\nfrom google.cloud import bigquery\nimport os\nfrom helpers import hs_helpers as hs_help\n\n########################################## BigQuery Connection\nbqclient = bigquery.Client()\n\ndef hubspot_replication():\n  \n  apikey = os.environ.get('hs_apikey')\n  \n  last_updated_deal = hs_help.get_last_updated_object(bqclient, 'deals', 'las_updated_column')\n  \n  recent_deals_list=hs_help.get_recent_modified(\"https://api.hubapi.com/deals/v1/deal/recent/modified?\",\n                                        apikey, count = 100, since=last_updated_deal*1000, max_results=100000)\n  \n  recent_deals = hs_help.clean_deals(recent_deals_list)\n  \n  example_schema = [\n    bigquery.SchemaField(\"amount_in_home_currency\", \"FLOAT\"),\n    bigquery.SchemaField(\"closed_lost_reason\", \"STRING\"),\n    bigquery.SchemaField(\"closedate\", \"DATE\"),\n    bigquery.SchemaField(\"days_to_close\", \"INTEGER\"))]\n \n  \n  hs_help.load_table_from_dataframe_safely(bqclient, recent_deals, 'project_name.dataset_name.table_name', example_schema)\n  \n  return 'Data pull complete'\n```\n:::\n\n\n> Note that you don't need to call the JSON with the Service Account key since you will define what Service Account will execute this function later\n\n## Setting scripts and routine in GCP\n\nThere are multiple options to deploy the function, the most straightforward is to upload a .zip from the console, you can find details [here](https://cloud.google.com/functions/docs/deploying)\n\nYou have two options to include the passwords and API keys that the function needs: [Using Secrets](https://cloud.google.com/functions/docs/configuring/secrets) (recommended) or set a [Environment Variables](https://cloud.google.com/functions/docs/configuring/env-var). Both are very easy to implement and can be configured from the console at deployment time\n\nAfter you try your function you can create a schedule that triggers the HTTP function\n\n## Handling duplicates\n\nBigQuery doesn't support index like RDBMS so we can add duplicates to a table, to avoid this in the reports you have two options:\n\n1- Drop records that appear in both new batch and persistent BQ table before uploading the batch. You can implement something like this before calling `load_table_from_dataframe_safely`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef drop_duplicates(bq_client, table_name, ids):\n  \n  collapsed_ids = ', '.join(map(str, ids))\n  \n  query = f\"\"\"DELETE hubspot.{table_name} WHERE id in ({collapsed_ids})\"\"\"\n\n  query_job = bq_client.query(query)\n  \n  return query_job.result()\n```\n:::\n\n\n2- Creating Views or Rewriting tables [removing duplicates with SQL sentence](https://cloud.google.com/bigquery/streaming-data-into-bigquery#dataconsistency)\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT\n  * EXCEPT(row_number)\nFROM (\n  SELECT\n    *,\n    ROW_NUMBER()\n          OVER (PARTITION BY ID_COLUMN) row_number\n  FROM\n    `TABLE_NAME`)\nWHERE\n  row_number = 1\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}