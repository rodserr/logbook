[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Visualize Running Routes\n\n\nExtracting data from Strava API to build route Visualizations\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial: Replicating data to BigQuery\n\n\nQuick Tutorial on how to automate data replication to BigQuery using Google Cloud Functions in order to build BI/ML tools\n\n\n\nFeb 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPL Standings - table contest 2021\n\n\nMaking a Selectable and Editable DT::DataTable in Shiny.\n\n\n\nOct 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFUTVEN-Monitor shinyapp\n\n\nDashboard Monitoring Venezuelan football players performing abroad.\n\n\n\nJul 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLiverpool 20-21 EPL Performance\n\n\nAnalysis of EPL 20/21 season of Liverpool FC using tidyverse\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/futven-monitor-shinyapp/index.html",
    "href": "posts/futven-monitor-shinyapp/index.html",
    "title": "FUTVEN-Monitor shinyapp",
    "section": "",
    "text": "Monitoring Venezuelan football players performing abroad, data owned by  obtained using rvest and worldfootballR\nData is stored in BigQuery tabels using bigrquery package\nThree panels exist in the app:\n\nEliminatorias: Basic stats of Venezuelan National team players perform in the Qatar 2022 World cup qualifying\nVenex: Goals, assists and minutes played in regular (current or last) season of active players performing abroad\nScout: Schedule of venex players for the next 7 days. Summary of performance in the last 7 days\n\nCheck  at https://rodserr.shinyapps.io/futven/.\nCode at https://github.com/rodserr/futven-monitor"
  },
  {
    "objectID": "posts/epl-standings-table-contest-2021/index.html",
    "href": "posts/epl-standings-table-contest-2021/index.html",
    "title": "EPL Standings - table contest 2021",
    "section": "",
    "text": "I will document the making of a selectable and editable DT::dataTable in shiny. This will also be my submission to participate on RStudio table contest for 2021. I will use understatr package to get_league_teams_stats for the 2021/2022 Premier League and build a standings table. This table will be selectable in order to display cumulative xG for the selected team, and editable to add comments for the team in a Total xG plot. Visit shiny here\n\nPackages\n\n# Libraries----\nlibrary(shiny)\nlibrary(understatr)\nlibrary(tidyverse)\nlibrary(DT) \nlibrary(ggrepel) # include automated positioned text in ggplot\nlibrary(ggimage) # include team logo in ggplot\nlibrary(shinycssloaders) # Nice spinner to show while loading outputs\nlibrary(bslib) # to themed the shiny\nlibrary(thematic) # to pass the bslib theme to ggplots\n\n\n\nGetting Data\nLet’s use understatr::get_league_teams_stats to retrieve results for every team in the 2021/2022 Premier League. team_logo.csv contains image addresses for the logo of every team\n\nepl_2021 &lt;- get_league_teams_stats(league_name = \"EPL\", year = 2021)\nteam_logo &lt;- read_csv('table-contest-2021/team_logo.csv')\n\nLet’s summarise epl_2021 and join the URL of the logos, which will be our standing table and also our dataframe to display Total xG plot. We also made the cumulative xG dataframe that will be team filtered every time we select a row in the standings table to display only the curve for the selected team\n\n# Standings\nepl_standings &lt;- epl_2021 %&gt;% \n    group_by(team_name) %&gt;% \n    summarise(\n        Loses = sum(loses),\n        Draws = sum(draws),\n        Wins = sum(wins),\n        Pts = sum(pts),\n        GD = sum(scored)-sum(missed),\n        Comment = '',\n        xG = sum(xG),\n        xGA = sum(xGA),\n        .groups = 'drop'\n    ) %&gt;% \n    arrange(desc(Pts)) %&gt;% \n    left_join(team_logo) %&gt;% \n    mutate(Team = sprintf('&lt;img src=\"%s\" height=\"28\"&gt;&lt;/img&gt;', logo)) %&gt;% \n    relocate(Team)\n\n# Cumulative xG\nroll_xG &lt;- epl_2021 %&gt;% \n    group_by(team_name) %&gt;%\n    arrange(team_name, date) %&gt;% \n    transmute(\n        match_day = 1:n(),\n        team_name,\n        xG = cumsum(xG),\n        xGA = cumsum(xGA),\n        G = cumsum(scored),\n        GA = cumsum(missed)\n    ) %&gt;% \n    ungroup() %&gt;% \n    pivot_longer(contains('G')) %&gt;% \n    mutate(type = if_else(name %in% c('G', 'GA'), 'real', 'expected'))\n\n\n\nSetting Reactivity and observes\nThe first thing we need to do is create a reactiveValues and set an observe to catch the value every time the source dataframe changes, in this case, it would not change since epl_standings is static, but this would be necessary if epl_standings were reactive. Also, we need to create a dataTableProxy to store the previous value for the main dataTable -‘standings’ is our DT::dataTable that we will create later on-\n\n# Create Empty Reactive Value\ntable_reactive_value &lt;- reactiveValues(df = NULL)\n\n# observe in case of epl_standings would be reactive\nobserve({\n  table_reactive_value$df &lt;- epl_standings\n})\n\n# Creating table proxy to store previous values\nstandings_proxy &lt;- dataTableProxy('standings')\n\nNext, we set the observeEvent that in the event of a cell edit, will update the value of the reactiveValue that stores the copy of the source dataframe. Note the event will always be an input of the nameOfTheDT_cell_edit\n\n# Update table\nobserveEvent(input$standings_cell_edit, {\n  info = input$standings_cell_edit\n  str(info)\n  \n  # Location and value of the edit\n  i = info$row\n  j = info$col \n  v = info$value\n  \n  # Coerce value of the same data type and replace proxy\n  table_reactive_value$df[i, j] &lt;- DT::coerceValue(v, pull(table_reactive_value$df[i, j]))\n  aux_table &lt;- table_reactive_value$df\n  replaceData(standings_proxy, aux_table, resetPaging = FALSE, rownames = T)\n})\n\nNow lets build the DT::dataTable. The only column that will not be selectable is Comment which is column number 8, so we build a matrix of nrow(epl_standings) rows and two columns that maps the cells of epl_standings that will not be selectable. Column one and column two indicates row number and column number respectively. All values are negative, in that way we specify the no selectable parts of the table\nThe editable range is easier to set with the disable argument. Note that there are 3 columns that we do not want to display in the table, we do not use dplyr::select to remove them, instead, we set columnDefs argument otherwise would be a mismatch between the reactiveValue and the dataTable columns\n\noutput$standings &lt;- DT::renderDataTable({\n  \n  ncolumns &lt;- ncol(epl_standings)\n  nrows &lt;- nrow(epl_standings)\n  unselectable_matrix &lt;- matrix(c(-(1:nrows), rep(-8, nrows)), ncol = 2)\n  \n  epl_standings %&gt;%\n    DT::datatable(\n      rownames = T,\n      style = 'bootstrap',\n      selection = list(mode = 'single', target = \"cell\", selectable = unselectable_matrix),\n      editable = list(target = \"cell\", disable = list(columns = 1:7)),\n      escape = F,\n      options = list(\n        searching = F,\n        paging = F,\n        info = F,\n        columnDefs = list(\n          list(visible=FALSE, targets = c(9,10,11)),\n          list(className = 'dt-center', targets = '_all')\n        )\n      )\n    )\n  \n})\n\nNow we use the reactiveValue to build the output with the dataframe edited, in this case, a ggplot\n\noutput$total_xG &lt;- renderPlot({\n  \n  table_reactive_value$df %&gt;%\n    ggplot(aes(x = xG, y = xGA, label = Comment)) +\n    geom_image(aes(image = logo), asp = 10/5, size = .04) +\n    geom_text_repel(box.padding = 1, max.overlaps = Inf, size = 4,\n                    min.segment.length = 0, segment.curvature = -.5) +\n    labs(x = 'Expected Goal (xG)', y = 'Expected Goal Against (xGA)')\n  \n})\n\n\n\nShowModal of selected row\nWe set a reactive that activates when a _cells_selected and stores the name of the team in the selected row. With the reactive team_selected we filter the dataframe and build the ggplot. Finally, we set the observeEvent that triggers the modal based on the dataTable_cells_selected\n\nteam_selected &lt;- reactive({\n  selected_team &lt;- epl_standings %&gt;% \n    slice(input$standings_cells_selected[1,1]) %&gt;% \n    pull(team_name)\n})\n\n# Modal to display cumulative xG\nobserveEvent(\n  input$standings_cells_selected,\n  {\n    if(nrow(input$standings_cells_selected)&gt;0){\n      showModal(\n        modalDialog(\n          title = paste0(team_selected(), \" Cumulative xG\"),\n          easyClose = T,\n          size = 'l',\n          plotOutput('rolling_xG')\n        )\n      ) \n    }\n  }\n)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Actuarial science grad turned data enthusiast! I leverage my analytical background across data analysis, science, and engineering. Lately, spatial data and mapping have piqued my interest.\nI build visualizations and interactive tools, empowering decision-making in finance, recruiting, and urban tech. Experience and a curious mind fuel my work, not just degrees. In this blog I share my projects, thoughts, and learnings."
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html",
    "href": "posts/liverpool-20-21-epl-performance/index.html",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "",
    "text": "On the last match of the 20/21 season, Liverpool celebrated their Champions League qualification, almost as if they had won a title. In mid-January the team fell into a negative streak, to the point of breaking the record for the most consecutive games losing at Anfield. In this post we will try to quantify this turbulent season using tidyverse. All data is extracted from understat through understatr package\nlibrary(tidyverse)\nlibrary(understatr)\nlibrary(ggsoccer)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(here)"
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#season-data",
    "href": "posts/liverpool-20-21-epl-performance/index.html#season-data",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Season Data",
    "text": "Season Data\nget_league_teams_stats function retrieve results at the season-team level. Let’s ask for the last three seasons and compare LFC rolling points. Note that understat labels the season with the first year of it, for example when we say 2020 season it would be 2020/2021\n\nepl &lt;- list(2020, 2019, 2018) %&gt;%\n  map_df(~get_league_teams_stats('EPL', year = .x))\n\nLet’s compare LFC runs in the last three seasons\n\n# Filter only LFC games\nmatch_season &lt;- epl %&gt;% filter(team_name == 'Liverpool')\n\n# Group by year and cumsum pts\nmatch_season %&gt;% \n  arrange(date) %&gt;%\n  group_by(year) %&gt;% \n  transmute(\n    cm_pts = cumsum(pts),\n    Matches = 1:n(),\n    .label = if_else(cm_pts == last(cm_pts), as.character(year), NA_character_)\n  ) %&gt;% \n  ggplot(\n    aes(x = Matches, y = cm_pts, color = as_factor(year), alpha = (year != 2020))\n  ) +\n  geom_line() +\n  ggrepel::geom_label_repel(aes(label = .label), nudge_x = 1, na.rm = TRUE) +\n  scale_alpha_manual(values = c(1, .3), guide = FALSE) +\n  theme(legend.position = 'none') +\n  labs(y = 'Points', title = 'Cumulative Points')\n\n\n\n\n\n\n\n\nThe gap increases after the 14th match, but it begins to recover around the 30th. 2018 and 2019 were amazing seasons, it would be hard to have that slope again\nunderstat provides expected goals xG, a very popular metric in soccer analytics. According to their page\n\n…more and more sports analytics turn to the advanced models like xG, which is a statistical measure of the quality of chances created and conceded… we trained neural network prediction algorithms with the large dataset (&gt;100,000 shots, over 10 parameters for each)\n\nLet’s see how many matches LFC couldn’t win having greater xG than his rival\n\nmatch_season %&gt;% \n  filter((xG &gt; xGA) & result != 'w') %&gt;% \n  group_by(year) %&gt;% \n  summarise(\n    `Nº Matches` = n(),\n    `Points` = 3*n()-sum(pts)\n  ) %&gt;% \n  gt() %&gt;% \n  tab_header(title = 'Missed Wins')\n\n\n\n\n\n\n\n\nMissed Wins\n\n\nyear\nNº Matches\nPoints\n\n\n\n\n2018\n3\n7\n\n\n2019\n4\n9\n\n\n2020\n8\n18\n\n\n\n\n\n\n\n\nThe number of matches doubles the previous two season, losing 18 points in games where LFC supposed to have an advantage"
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#match-data",
    "href": "posts/liverpool-20-21-epl-performance/index.html#match-data",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Match Data",
    "text": "Match Data\nWe can ask for stats of every LFC player at season level\n\n# Get players stats\nplayer_season &lt;- list(2020, 2019, 2018) %&gt;% \n  map_df(~get_team_players_stats('Liverpool', year = .x))\n\nunderstatr also provides shots and stats for single matches, but we need to know each match_id, which appears only on player-match level data. Let’s find all match_ids for LFC 2020 season by looking at games where Fabinho, Jota and Thiago played. Later in the post we will analyze these players individually\n\n# Get games from players\nplayer_match &lt;- list(6854, 229, 3420) %&gt;% \n  map_df(~get_player_matches_stats(.x))\n\n# Distinct matches ids from 2020 season\nmatch_ids &lt;- player_match %&gt;%\n  filter(year == 2020) %&gt;%\n  distinct(match_id) %&gt;%\n  pull()\n\n# Get shots from every game\nmatch_shots &lt;- map_df(match_ids, ~get_match_shots(.x))\n\n# Get stats from every game\nmatch_stats &lt;- map_df(match_ids, ~get_match_stats(.x))"
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#injured-defense",
    "href": "posts/liverpool-20-21-epl-performance/index.html#injured-defense",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Injured Defense",
    "text": "Injured Defense\nLFC’s main problems this season were due to the injuries of their 3 main Central defenses early on the season. Let’s check DC performances with match stats data and join it with shots/goals conceded\n\n# Shots conceded by LFC \nshots_against &lt;- match_shots %&gt;% \n  filter((h_team != 'Liverpool' & h_a == 'h') | (a_team != 'Liverpool' & h_a == 'a')) %&gt;% \n  filter(result != 'OwnGoal') %&gt;% \n  group_by(match_id) %&gt;% \n  summarise(\n    shots = n(),\n    goals_against = sum(result == 'Goal')\n  )\n\n# DC pairs from every 2020 game\ncentral_def &lt;- match_stats %&gt;% \n  filter(team_id == 87, position == 'DC')\n\n# Paste players names & Join with shots conceded\ncentral_def_pairs &lt;- central_def %&gt;% \n  group_by(match_id) %&gt;% \n  arrange(player) %&gt;% # So player order do not affect \n  dplyr::summarise(pair = paste0(player, collapse = ' - ')) %&gt;% \n  left_join(shots_against, by = 'match_id')\n\n# Distinct pairs\nn_pairs &lt;- unique(central_def_pairs$pair) %&gt;% length()\n\n# Individual DC with most appears\ndc_more_appears &lt;- central_def %&gt;% count(player) %&gt;% slice_max(order_by = n)\n\n# Build gtable\ncentral_def_pairs %&gt;% \n  group_by(pair) %&gt;% \n  summarise(\n    appearances = n(), \n    shots = sum(shots), \n    goals = sum(goals_against),\n    shots_per_game = round(shots/appearances, 1),\n    goals_per_game = round(goals/appearances, 1)\n  ) %&gt;% \n  filter(appearances &gt; 1) %&gt;% \n  arrange(desc(appearances)) %&gt;%\n  gt(rowname_col = 'pair') %&gt;% \n  tab_header(\n    title = 'Goals/Shots Conceded by Pair DC', \n    subtitle = glue::glue('Pairs with &gt;1 appearence of {n_pairs} pairs used in the season')\n  ) %&gt;% \n  tab_footnote(\n    locations = cells_column_labels(vars(appearances)),\n    footnote = glue::glue('{dc_more_appears$player} had the highest number of appearances with {dc_more_appears$n}')\n  ) %&gt;% \n  cols_align('center')\n\n\n\n\n\n\n\n\nGoals/Shots Conceded by Pair DC\n\n\nPairs with &gt;1 appearence of 16 pairs used in the season\n\n\n\nappearances1\nshots\ngoals\nshots_per_game\ngoals_per_game\n\n\n\n\nFabinho - Joel Matip\n6\n46\n3\n7.7\n0.5\n\n\nNathaniel Phillips - Rhys Williams\n6\n65\n4\n10.8\n0.7\n\n\nNathaniel Phillips - Ozan Kabak\n4\n30\n1\n7.5\n0.2\n\n\nFabinho - Jordan Henderson\n3\n23\n5\n7.7\n1.7\n\n\nFabinho - Ozan Kabak\n3\n30\n3\n10.0\n1.0\n\n\nJoseph Gomez - Virgil van Dijk\n3\n28\n11\n9.3\n3.7\n\n\nFabinho - Nathaniel Phillips\n2\n19\n1\n9.5\n0.5\n\n\nJordan Henderson - Nathaniel Phillips\n2\n21\n2\n10.5\n1.0\n\n\nJordan Henderson - Ozan Kabak\n2\n20\n5\n10.0\n2.5\n\n\n\n1 Fabinho had the highest number of appearances with 17\n\n\n\n\n\n\n\n\n\nWow!! 16 different pairs of DC across the season. Starting pair Virgil-Gomez had only 3 appearances and have the most goals per game conceded due to the unfortunate loss against Aston Villa (7-2). Even Henderson had to play DC which is clearly not his position. Looks like Phillips and Kabak did a pretty good job having the minimum shots and goals conceded, though Kabak also suffered an injury at the end of the season. Fabinho had the most appearances despite not being DC which takes us to the next key point."
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#lossing-fabinho-in-the-midfield",
    "href": "posts/liverpool-20-21-epl-performance/index.html#lossing-fabinho-in-the-midfield",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Lossing Fabinho in the midfield",
    "text": "Lossing Fabinho in the midfield\nI think Fabinho is one of the keys to the success of the last season, and losing him in the midfield weakened the LFC game.\n\nfabinho &lt;- player_match %&gt;% \n  filter(player_name == 'Fabinho')\n\n# Fabinho influence\nfab_perc_pts &lt;- fabinho %&gt;% \n  filter(year == 2020) %&gt;% \n  mutate(\n    # Compute pts earned by liverpool\n    pts = case_when(\n      h_team == 'Liverpool' & h_goals &gt; a_goals ~ 3, \n      a_team == 'Liverpool' & h_goals &lt; a_goals ~ 3, \n                              h_goals == a_goals ~ 1,\n                                              T ~ 0\n    )\n  ) %&gt;% \n  filter(position != 'Sub') %&gt;% \n  group_by(position) %&gt;% \n  summarise(\n    pts = sum(pts)/(n()*3),\n    games = n()\n  ) %&gt;% \n  ggplot(aes(x = position, y = pts, fill = position, label = glue::glue('{games} games played'))) +\n  geom_col() +\n  geom_text(nudge_y = .05, size = 2.5) +\n  scale_y_continuous(labels = scales::percent) +\n  theme(legend.position = 'none') +\n  labs(x = '', y = '% of pts Earned', title = '% Points earned')\n\n# LFC Cumulative pts color by Fabinho position \nfab_cml_pts &lt;- match_season %&gt;% \n  filter(year == 2020) %&gt;% \n  left_join(fabinho %&gt;% filter(year == 2020), by = 'date') %&gt;% \n  arrange(date) %&gt;% \n  mutate(\n    cm_pts = cumsum(pts),\n    Matches = 1:n(),\n    position = replace_na(position, 'Injured')\n  ) %&gt;% \n  ggplot(aes(x = Matches, y = cm_pts, color = position)) +\n  geom_point() +\n  theme(\n    legend.position = 'top',\n    legend.justification = 'left',\n    legend.title = element_blank()\n  ) +\n  labs(y = 'Points', title = 'LFC Cumulative Points', subtitle = 'Colored by Fabinho position')\n\n# Patchwork join\nfab_perc_pts + fab_cml_pts + \n  plot_layout(widths = c(1, 3)) +\n  plot_annotation(title = 'Fabinho Influence by Position')\n\n\n\n\n\n\n\n\nWhen Fabinho plays DC LFC won only 49% of possible points, while as MC the number rises to 85%. You can see how the flattest runs happen at mid-season when he played as DC and even when he was injured (yes, he was injured too). And look at the comeback at the end of the season, he played as MC in all those victories… Yes, Klopp! show some love to that man!"
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#goals-shortfall",
    "href": "posts/liverpool-20-21-epl-performance/index.html#goals-shortfall",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Goals Shortfall",
    "text": "Goals Shortfall\nInjuries weren’t the only problem, goals shortfall also affected the performance\n\n# Attackers\nmain_FW &lt;- player_season %&gt;% \n  filter(player_id %in% c(1250, 838, 482))\n\n# Attackers performances\nmain_FW %&gt;% \n  transmute(\n    player_name, year,\n    Scored = ((npg+assists)/time)*90,\n    Expected = ((npxG+xA)/time)*90\n  ) %&gt;%\n  pivot_longer(c(Scored, Expected)) %&gt;% \n  ggplot(aes(x = year, y = value, color = name)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = c(2018, 2019, 2020)) +\n  facet_wrap(~player_name) +\n  theme(\n    legend.position = 'top',\n    legend.justification = 'left',\n    legend.title = element_blank()\n  ) +\n  labs(title = 'No penalty Goals + Assists per 90 minutes', y = 'G + A per 90m', x = '')\n\n\n\n\n\n\n\n\nAll three FW decrease their G+A per game in respect of their previous seasons, Especially Salah and Mane. Let’s compare the top 5 contributors across seasons\n\n# Main contributors by season\nplayer_season %&gt;% \n  mutate(\n    GA_90m = ((npg+assists)/time)*90,\n    n = tidytext::reorder_within(player_name, GA_90m, year)\n  ) %&gt;% \n  group_by(year) %&gt;% \n  slice_max(order_by = GA_90m, n = 5) %&gt;% \n  ggplot(aes(x = GA_90m, y = n)) +\n  geom_col() +\n  facet_wrap(~year, scales = 'free_y') +\n  tidytext::scale_y_reordered() +\n  labs(title = 'Top Contributors: G + A per 90 minutes', x = 'G + A per 90m', y = '')\n\n\n\n\n\n\n\n\nJota and Chamberlain played a big role in helping to fill the goal gap this season, though they both suffered major injuries that made them miss a lot of games\nLet’s play around with the shots data to compare the top 5 shooters\n\n# All LFC shots\nlfc_shots &lt;- match_shots %&gt;% \n  filter((h_team == 'Liverpool' & h_a == 'h') | (a_team == 'Liverpool' & h_a == 'a'))\n\n# Top 5 shooters\n.shooters &lt;- lfc_shots %&gt;% \n  count(player_id) %&gt;% \n  slice_max(order_by = n, n = 5) %&gt;% \n  pull(player_id)\n\n# Build df for automagic_tabs2\nshooters_dataset &lt;- lfc_shots %&gt;% \n  filter(player_id %in% .shooters) %&gt;% \n  group_by(player) %&gt;% \n  nest() %&gt;% \n  mutate(\n    shotplot = map(\n      data,\n      function(df){\n        shots &lt;- df %&gt;%\n          mutate(.x = X*100, .y = 100-Y*100)\n        \n        pitch_text &lt;- shots %&gt;%\n          summarise(\n            .x = 65,\n            `Shots per Goal` = n()/sum(result == 'Goal'),\n            `Average xG` = mean(xG)\n          ) %&gt;% \n          pivot_longer(-.x) %&gt;% \n          mutate(\n            .y = c(25, 75),\n            value = round(value, 2)\n          )\n        \n        shots %&gt;% \n          filter(shotType != 'OtherBodyPart') %&gt;% #To mantain consistency over color groups\n          ggplot(aes(x = .x, y = .y)) +\n          annotate_pitch() +\n          theme_pitch() +\n          geom_point(aes(size = xG, color = shotType, alpha = result == 'Goal')) +\n          geom_text(data = pitch_text, aes(label = glue::glue('{name}: {value}')), size = 3.5) +\n          scale_size_continuous(limits = c(0, 1), breaks = c(.3, .6, .9)) +\n          scale_alpha_manual(values = c(.3, 1), guide = FALSE) +\n          coord_flip(xlim = c(60, 100), ylim = c(0, 100))\n      }\n    ),\n    n_shots = map_dbl(data, ~nrow(.x))\n  ) %&gt;% \n  arrange(desc(n_shots)) %&gt;% \n  ungroup()\n\nInspired by (@soccerPlots) For this, let’s use ggsoccer to set the soccer pitch as the background of the plot and automagic_tabs2 function from sknifedatar (@automagic_tabs), that automates the process of building tabs.\nThis solution works for .rmd files. check this so post for an alternative approach for .qmd files\n\nMohamed SalahSadio ManéRoberto FirminoTrent Alexander-ArnoldDiogo Jota\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nSalah is the one that makes more shots and needed less to score, he also has better xG from shots coming from the right side and he barely tries his right foot. Firmino and Mane are more balanced, trying shots with their weak foot and head, but they need more shots to score. It seems like Jota is the one that better chooses his shots. Trent has it more difficult because he shots from away having lower xG."
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#signning-performance",
    "href": "posts/liverpool-20-21-epl-performance/index.html#signning-performance",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Signning Performance",
    "text": "Signning Performance\nLet’s check the performance of summer signings Jota and Thiago\n\nplayer_match %&gt;%\n  filter(player_id %in% c(6854, 229), year %in% c(2020, 2019, 2018)) %&gt;%\n  mutate(\n    GA = npg+assists,\n    xGA = npxG+xA\n  ) %&gt;% \n  group_by(player_name, year) %&gt;% \n  summarise(\n    `Possible Time Played (%)` = sum(time)/(n()*90),\n    `G+A per 90m` = (sum(GA)/sum(time))*90,\n    `Key Passes per 90m` = (sum(key_passes)/sum(time))*90\n  ) %&gt;% \n  pivot_longer(-c(player_name, year)) %&gt;% \n  ggplot(aes(x = year, y = value, color = player_name)) +\n  geom_line() +\n  geom_point() +\n  facet_grid(player_name~name, scales = 'free') +\n  theme(legend.position = 'none') +\n  scale_x_continuous(breaks = c(2018, 2019, 2020)) +\n  labs(x = '', y = '', title = 'Summer Signning Performance')\n\n\n\n\n\n\n\n\nYou can see Diogo adapted very quickly, increasing G+A ratio of the last season, contrary to what happened with Thiago, that got better performance at the end of the season, being a key player in the midfield"
  },
  {
    "objectID": "posts/liverpool-20-21-epl-performance/index.html#hoodability",
    "href": "posts/liverpool-20-21-epl-performance/index.html#hoodability",
    "title": "Liverpool 20-21 EPL Performance",
    "section": "Hoodability",
    "text": "Hoodability\nIn this post (@Hoodability) were introduces the term of gaining points against the big six and loss against the bottom table. Let’s check hoodability for this season\n\n# Build Table\nposition_table &lt;- epl %&gt;% \n  filter(year == 2020) %&gt;% \n  group_by(team_name) %&gt;% \n  summarise(pts = sum(pts)) %&gt;% \n  arrange(desc(pts)) %&gt;% \n  transmute(team_name, position = 1:n())\n\n# pts earned by LFC\nmatch_shots %&gt;% \n  distinct(h_team, a_team, h_goals, a_goals) %&gt;% \n  transmute(\n    team_name = if_else(h_team == 'Liverpool', a_team, h_team),\n    pts =  case_when(\n      h_team == 'Liverpool' & h_goals &gt; a_goals ~ 3, \n      a_team == 'Liverpool' & h_goals &lt; a_goals ~ 3, \n      h_goals == a_goals ~ 1,\n      T ~ 0\n    )) %&gt;% \n  left_join(position_table, by = 'team_name') %&gt;% \n  mutate(\n    position_group = case_when(\n      position &lt;= 6 ~ 'Top 6',\n      position &gt;= 14 ~ 'Bottom 6',\n      T ~ 'Middle table'\n    )\n  ) %&gt;% \n  group_by(position_group) %&gt;% \n  summarise(\n    pts = sum(pts),\n    ppts = pts/(3*n())\n  ) %&gt;% \n  ggplot(aes(x = position_group, y = ppts, label = glue::glue('Total pts earned: {pts}'))) +\n  geom_col() +\n  geom_text(nudge_y = .05) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(y = 'Possible Points', x = 'Table Position', title = 'Possible Points Earned by Liverpool against')\n\n\n\n\n\n\n\n\nPretty much the same possible points were earned at the bottom and top of the table, but you would expect to have a better performance against the bottom 6 teams\nSooooo, the 20-21 season was a tough one for LFC and we knew it the moment Virgil surgery was confirmed, but it got worse with Gomez and Matip injuries because basically, we run out of DC. Despite this, the team managed to qualify for Champions League with an amazing comeback and a memorable moment in Allison’s 95’ header against West Brom. Hopefully next season’s analysis I will include more Klopp gifs running towards “the Kop”… In the meantime"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html",
    "href": "posts/replicating-data-to-bigquery/index.html",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "",
    "text": "BigQuery can be used as Data Warehouse to gather multiple silos and build BI tools or ML models. This tutorial is very general, for details check Google documentation. I’m recording my steps using Hubspot as the source, overall it would be something like:\n1- Write a function that extracts data from Hubspot (or any other source that can be pragmatically scheduled), and load it to BigQuery\n2- Upload the script to Google Cloud Functions and set a routine with Cloud Scheduler\n3- Handle duplicates"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html#create-a-service-account",
    "href": "posts/replicating-data-to-bigquery/index.html#create-a-service-account",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "Create a Service Account",
    "text": "Create a Service Account\nFirst, you will have to create a project and set the service account permission to the resources, at least these three: BigQuery, Functions, and Scheduler. For local development, you will need to download the Key as JSON file and place it in your project root\n\nfrom google.cloud import bigquery\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/.secrets/project_id.json\"\nbqclient = bigquery.Client()"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html#plan-the-schema-and-relations",
    "href": "posts/replicating-data-to-bigquery/index.html#plan-the-schema-and-relations",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "Plan the schema and relations",
    "text": "Plan the schema and relations\nSketch the relation between tables, datasets and be sure about data types. I usually define a dataset for every silo (data source)"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html#set-the-pipeline",
    "href": "posts/replicating-data-to-bigquery/index.html#set-the-pipeline",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "Set the Pipeline",
    "text": "Set the Pipeline\nAfter replicating the first batch of data we need to define how we will update the tables. Basically, every X minutes, hours, or days, depending on the frequency of the update, take the datetime of the last updated (or created) record saved in BQ, and query records created/updated in the source after that timestamp.\n\ndef get_last_updated_object(bq_client, table_name, field_name):\n  \n  \"\"\"\n  bq_client: BigQuery Client\n  table_name: str table to query\n  field_name: str name of the date/datetime field to get max value\n  \n  return: max field_name in table_name in unix\n  \"\"\"\n  \n  checkpoint_query = f\"SELECT max({field_name}) as last_updated FROM `poject_name.hubspot.{table_name}` \"\n  checkpoints_df = bq_client.query(checkpoint_query, project='project_name').to_dataframe()\n  \n  last_updated_datetime = checkpoints_df['last_updated'][0]\n  \n  last_updated_unix = dt.datetime.timestamp(last_updated_datetime)\n  \n  return last_updated_unix.__round__()"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html#compressing-an-etl-task-into-a-single-function",
    "href": "posts/replicating-data-to-bigquery/index.html#compressing-an-etl-task-into-a-single-function",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "Compressing an ETL task into a single function",
    "text": "Compressing an ETL task into a single function\nThe idea is to extract the data, transform into dataframes, and append it to the corresponding BQ table. We will query HubSpot’s deals as an example\n\nExtract\nHubspot API uses a pagination system for querying, so all this function does is retrieve data from a timestamp. The timestamp serves as a checkpoint, we will load only the batch of data after that checkpoint.\n\ndef get_recent_modified(url, hapikey, count, since, max_results):\n  \n  \"\"\"\n  url: str endpoint to retreive. One of deals, companies or engagements\n  hapikey: str API key\n  count: dbl number of object to retreive in a single call\n  since: dbl unix datetime in miliseconds to retreive after\n  max_results: dbl max number of records to retreive\n  \n  return: list with response\n  \"\"\"\n  \n  object_list = []\n  get_recent_url = url\n  parameter_dict = {'hapikey': hapikey, 'count': count, 'since': since}\n  headers = {}\n  \n  # Paginate your request using offset\n  has_more = True\n  while has_more:\n    parameters = urllib.parse.urlencode(parameter_dict)\n    get_url = get_recent_url + parameters\n    r = requests.get(url= get_url, headers = headers)\n    response_dict = json.loads(r.text)\n    has_more = response_dict['hasMore']\n    object_list.extend(response_dict['results'])\n    parameter_dict['offset'] = response_dict['offset']\n    \n    if len(object_list) &gt;= max_results: # Exit pagination, based on whatever value you've set your max results variable to.\n      print('maximum number of results exceded')\n      break\n  \n  print(f'Done!! Found {len(object_list)} object')\n  \n  return object_list\n\n\n\nTransform\nDo the necessary wrangling to convert the data to a ready-to-load pd.DataFrame, and make sure about column types. In this case, I extract some association id attributes from the list response and some attributes that are in another field of the list response, I merged and convert them to the correct field types.\n\ndef extract_value(new_objects, column_names):\n  \n  \"\"\"\n  new_objects: list with http response\n  column_names: list with column names to keep\n  \n  return: pd.DataFrame with column_names fields\n  \"\"\"\n  \n  has_associations = 'associations' in new_objects[0].keys()\n  \n  list_properties = []\n  for obj in new_objects:\n    \n    if has_associations:\n      associatedCompanyIds = obj['associations']['associatedCompanyIds']\n      associatedVids = obj['associations']['associatedVids']\n    \n    props = obj['properties']\n    \n    saved_properties = {}\n    for key, value in props.items():\n      saved_properties[key] = value['value']\n    \n    if has_associations:\n      saved_properties['associatedCompanyIds'] = associatedCompanyIds\n      saved_properties['associatedVids'] = associatedVids\n      \n    list_properties.append(saved_properties)\n  \n  df_properties = pd.DataFrame(list_properties)\n  \n  subset_columns = df_properties.columns.intersection(set(column_names))\n  \n  df_properties = df_properties[subset_columns]\n  \n  if has_associations:\n    df_properties['associatedVids'] = df_properties['associatedVids'].apply(lambda x: '-'.join(map(str, x)))\n    df_properties = df_properties.explode('associatedCompanyIds')\n    \n  return df_properties\n\n\n\ndef parse_properties(df, columns_to_datetime, columns_to_numeric, columns_to_boolean):\n  \n  \"\"\"\n  df: pd.DataFrame\n  columns_to_datetime, columns_to_numeric, columns_to_boolean: list with names of the columns to parse\n  \n  return: pd.DataFrame with parsed columns\n  \"\"\"\n  \n  if columns_to_datetime:\n    df[columns_to_datetime] = df[columns_to_datetime].apply(pd.to_datetime, unit =  'ms')\n  \n  if columns_to_numeric:\n    df[columns_to_numeric] = df[columns_to_numeric].apply(pd.to_numeric, errors = 'coerce')\n    \n  if columns_to_boolean:\n    df[columns_to_boolean] = df[columns_to_boolean].replace({'true': True, 'false': False}).astype('boolean')\n  \n  return df\n\n\ndef clean_deals(deals_list):\n  \n  \"\"\"\n  deals_list: list with http response\n  \n  return: pd.DataFrame with necessary columns and correct types\n  \"\"\"\n  \n  deal_properties = ['amount_in_home_currency', 'closed_lost_reason', 'closedate', 'createdate', 'days_to_close', 'deal_currency_code', \n                     'dealname', 'dealstage', 'dealtype', 'hs_is_closed', 'hs_is_closed_won', 'hs_lastmodifieddate', 'hs_object_id', \n                     'associatedCompanyIds', 'associatedVids', 'pipeline']\n  \n  deals = extract_value(deals_list, deal_properties)\n  deals = parse_properties(\n    deals,\n    columns_to_datetime = ['createdate', 'closedate', 'hs_lastmodifieddate'],\n    columns_to_numeric = ['days_to_close', 'hs_object_id', 'amount_in_home_currency'],\n    columns_to_boolean = ['hs_is_closed', 'hs_is_closed_won']\n    )\n    \n  return deals\n\n\n\nLoad\nUse the BigQuery API to load_table_from_dataframe(), previously define the schema to avoid data type errors between the upload batch and the BQ table.\n\n# Example Schema\nexample_schema = [\n  bigquery.SchemaField(\"amount_in_home_currency\", \"FLOAT\"),\n  bigquery.SchemaField(\"closed_lost_reason\", \"STRING\"),\n  bigquery.SchemaField(\"closedate\", \"DATE\"),\n  bigquery.SchemaField(\"days_to_close\", \"INTEGER\"))]\n  \n  \ndef load_table_from_dataframe_safely(bq_client, df: pd.DataFrame, table_id: str, table_schema: list):\n    \n    \"\"\"\n    df: dataframe to upload to BigQuery\n    table_id: id of table in BigQuery, in should consist of project.dataset.table\n    table_schema: list of .SchemaField element for every column in the table\n    \n    return: nothing, it uploads the df to BQ in the table_id destination\n    \"\"\"\n    \n    if df.empty:\n        return(print(f'Empty Table, there are not new records in {table_id}'))\n    else:\n        job_config = bigquery.LoadJobConfig(schema=table_schema)\n        job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_config)\n        return(job.result())\n\n\n\nInclude all into a single function\nCloud function executes a function, not a script, so all the helpers functions need to be used in one function. There are three places where helpers can live:\n\nIn the same script as the main function (you will define what is the function that GCP will execute, so it doesn’t matter to have variables and helpers in the same script)\nIn a different script that lives in a folder at the same level directory that main.py\nBuild a library and add it to dependencies\n\nI think 3rd one is the best choice but adds complexity to the process, so we will go with the 2nd one to be able to use the helpers in local development\nThe final main.py will be like\n\n########################################## Libraries\nimport pandas as pd\nimport datetime as dt\nfrom google.cloud import bigquery\nimport os\nfrom helpers import hs_helpers as hs_help\n\n########################################## BigQuery Connection\nbqclient = bigquery.Client()\n\ndef hubspot_replication():\n  \n  apikey = os.environ.get('hs_apikey')\n  \n  last_updated_deal = hs_help.get_last_updated_object(bqclient, 'deals', 'las_updated_column')\n  \n  recent_deals_list=hs_help.get_recent_modified(\"https://api.hubapi.com/deals/v1/deal/recent/modified?\",\n                                        apikey, count = 100, since=last_updated_deal*1000, max_results=100000)\n  \n  recent_deals = hs_help.clean_deals(recent_deals_list)\n  \n  example_schema = [\n    bigquery.SchemaField(\"amount_in_home_currency\", \"FLOAT\"),\n    bigquery.SchemaField(\"closed_lost_reason\", \"STRING\"),\n    bigquery.SchemaField(\"closedate\", \"DATE\"),\n    bigquery.SchemaField(\"days_to_close\", \"INTEGER\"))]\n \n  \n  hs_help.load_table_from_dataframe_safely(bqclient, recent_deals, 'project_name.dataset_name.table_name', example_schema)\n  \n  return 'Data pull complete'\n\n\nNote that you don’t need to call the JSON with the Service Account key since you will define what Service Account will execute this function later"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html#setting-scripts-and-routine-in-gcp",
    "href": "posts/replicating-data-to-bigquery/index.html#setting-scripts-and-routine-in-gcp",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "Setting scripts and routine in GCP",
    "text": "Setting scripts and routine in GCP\nThere are multiple options to deploy the function, the most straightforward is to upload a .zip from the console, you can find details here\nYou have two options to include the passwords and API keys that the function needs: Using Secrets (recommended) or set a Environment Variables. Both are very easy to implement and can be configured from the console at deployment time\nAfter you try your function you can create a schedule that triggers the HTTP function"
  },
  {
    "objectID": "posts/replicating-data-to-bigquery/index.html#handling-duplicates",
    "href": "posts/replicating-data-to-bigquery/index.html#handling-duplicates",
    "title": "Tutorial: Replicating data to BigQuery",
    "section": "Handling duplicates",
    "text": "Handling duplicates\nBigQuery doesn’t support index like RDBMS so we can add duplicates to a table, to avoid this in the reports you have two options:\n1- Drop records that appear in both new batch and persistent BQ table before uploading the batch. You can implement something like this before calling load_table_from_dataframe_safely:\n\ndef drop_duplicates(bq_client, table_name, ids):\n  \n  collapsed_ids = ', '.join(map(str, ids))\n  \n  query = f\"\"\"DELETE hubspot.{table_name} WHERE id in ({collapsed_ids})\"\"\"\n\n  query_job = bq_client.query(query)\n  \n  return query_job.result()\n\n2- Creating Views or Rewriting tables removing duplicates with SQL sentence\n\nSELECT\n  * EXCEPT(row_number)\nFROM (\n  SELECT\n    *,\n    ROW_NUMBER()\n          OVER (PARTITION BY ID_COLUMN) row_number\n  FROM\n    `TABLE_NAME`)\nWHERE\n  row_number = 1"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html",
    "href": "posts/strava-route-visualization/index.html",
    "title": "Visualize Running Routes",
    "section": "",
    "text": "This is a tutorial-like post to practice data manipulation, visualization and API principles. There has been one year since I started running in a weekly basis, and recently I completed my longest run so far (25km). I log my runs in Strava which has an API with free features that let you get some stats and even route points. Lets build a visualization of this run.\nknitr::opts_chunk$set(\n  eval = TRUE,\n  message = FALSE,\n  warning = FALSE,\n  dev = \"ragg_png\",\n  dpi = 300\n)\n\n.post_path &lt;- 'posts/strava-route-visualization'\nhere::i_am(glue::glue('{.post_path}/index.qmd'))\nrenv::use(lockfile = here::here(.post_path, \"renv.lock\"))\nlibrary(dplyr) # Data manipulation\nlibrary(httr2) # Make HTTP requests\nlibrary(jsonlite) # Handle json files\nlibrary(ggmap) # Mapping geometries\nlibrary(purrr) # Manipulate list elegantly\nlibrary(ggplot2) # Visualizations\nlibrary(here) # Working directory helper\nlibrary(systemfonts) # Work with fonts\nlibrary(ragg) # Work with fonts"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html#get-strava-data",
    "href": "posts/strava-route-visualization/index.html#get-strava-data",
    "title": "Visualize Running Routes",
    "section": "GET Strava Data",
    "text": "GET Strava Data\nStrava API use OAuth authentication protocol. Hadley has an article that explain how to connect OAuth from R using httr2\nBasically it consist in get credentials (client ID and secrets) and create the client to perform the authentication flow.\n\nclient &lt;- httr2::oauth_client(\n  id = Sys.getenv('STRAVA_CLIENT_ID'),\n  secret = Sys.getenv('STRAVA_CLIENT_SECRET'),\n  token_url = \"https://www.strava.com/oauth/token\",\n  name = \"strava-oauth-test\"\n)\n\n# Check client is set correctly\noauth &lt;- httr2::oauth_flow_auth_code(\n  client = client,\n  auth_url = 'https://www.strava.com/oauth/authorize',\n  scope = 'activity:read'\n)\n\nSys.setenv(\"STRAVA_ACCESS_TOKEN\" = oauth$access_token)\n\nActivity ID is explicit in the url of the strava webpage for the required activtity. We hit the /activity endpoint setting the activity target id\n\nactivity &lt;- request(\"https://www.strava.com/api/v3/activities/11886025399\") %&gt;% \n  httr2::req_auth_bearer_token(\n    Sys.getenv(\"STRAVA_ACCESS_TOKEN\")\n  ) %&gt;%\n  httr2::req_url_query(include_all_efforts = T) %&gt;% # Include Splits and efforts\n  req_perform() %&gt;% \n  resp_body_json()"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html#bulding-tables",
    "href": "posts/strava-route-visualization/index.html#bulding-tables",
    "title": "Visualize Running Routes",
    "section": "Bulding tables",
    "text": "Bulding tables\nWe got metrics for the activity, like total distance, moving time, elevation, etc.\n\nmetrics &lt;- activity %&gt;% \n  keep_at(c('distance', 'moving_time', 'total_elevation_gain', 'average_heartrate')) %&gt;% \n  flatten_dfc() %&gt;% \n  mutate(\n    moving_time = moving_time/60,\n    distance = distance/1000,\n    pace = moving_time/distance\n  )\n\nmetrics\n\n# A tibble: 1 × 5\n  distance moving_time total_elevation_gain average_heartrate  pace\n     &lt;dbl&gt;       &lt;dbl&gt;                &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1     25.3        155.                 75.2              147.  6.11\n\n\nWe want to plot the route of the run, so we need to extract the polyline. This comes encoded with google algorithm, we can use googlePolylines package to decode it.\n\nroute &lt;- activity %&gt;% \n  pluck('map', 'polyline', 1) %&gt;% \n  googlePolylines::decode() %&gt;% \n  pluck(1) %&gt;% \n  mutate(\n    dist = distHaversine(\n      cbind(lon, lat),\n      cbind(lag(lon), lag(lat))\n    ),\n    distcum = cumsum(tidyr::replace_na(dist, 0))\n  )\n\n\nbbox &lt;- make_bbox(lon, lat, data = route)\nmap &lt;- get_stadiamap( bbox = bbox, maptype = \"stamen_toner_lite\", zoom = 14 )\n\n\nstart &lt;- route %&gt;%\n  slice_head(n = 1) %&gt;% \n  mutate(label_ = 'start')\n\nstart_arrow &lt;- route %&gt;%\n  filter(\n    row_number() %in% c(1, 6)\n  ) %&gt;% \n  select(lat, lon) %&gt;% \n  mutate(\n    lat = lat+0.0015,\n    aux = c('start', 'end')\n  ) %&gt;% \n  tidyr::pivot_wider(names_from = aux, values_from = c(lat, lon))\n\n\nsplits &lt;- activity$splits_metric %&gt;% \n  map(~flatten(.x) %&gt;% as_tibble) %&gt;% \n  list_rbind() %&gt;% \n  mutate(\n    pace = (lubridate::today() + lubridate::seconds_to_period(moving_time)) %&gt;% \n      format(format = '%M:%S') %&gt;% \n      stringr::str_remove('^0')\n  )\n\nbest_split &lt;- splits %&gt;% \n  filter(distance == 1000) %&gt;% # Omit final split\n  slice_min(order_by = moving_time, n = 1, with_ties = FALSE)\n\nbest_split_coord &lt;- route %&gt;% \n  mutate(\n    closest_coord = abs(distcum-(best_split$split*1000))\n  ) %&gt;% \n  slice_min(order_by = closest_coord, n = 1, with_ties = FALSE) %&gt;% \n  mutate(\n    pace = best_split$pace,\n    avg_heartrate = best_split$average_heartrate,\n    label_ = glue::glue('&lt;span style=\"font-size:7pt;\"&gt;*end of*&lt;/span&gt; **Fastest Kilometer**&lt;br&gt;Pace: {pace}&lt;br&gt;Avg HeartRate: {round(avg_heartrate, 0)} bpm')\n  )"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html#plot",
    "href": "posts/strava-route-visualization/index.html#plot",
    "title": "Visualize Running Routes",
    "section": "Plot",
    "text": "Plot\nhttps://www.cararthompson.com/posts/2024-01-12-using-fonts-in-r-for-dataviz/2024-01-12_getting-fonts-to-work\n\n.linecolor &lt;- '#fc4c02'\n\nggmap(map) +\n\n  # Path (route)\n  geom_path(\n    data = route,\n    color = .linecolor,\n    linewidth = 2.5,\n    aes(x = lon, y = lat)\n  ) +\n  geom_path(\n    data = route,\n    color = 'white',\n    linewidth = 0.2,\n    aes(x = lon, y = lat)\n  ) +\n  \n  # Start-Finish points\n  geom_point(\n    data = start,\n    size = 4, color = .linecolor,\n    aes(x = lon, y = lat)\n  ) +\n  \n  # Start-Finish Arrow Labels\n  geom_segment(\n    data = start_arrow,\n    color = .linecolor,\n    linewidth = 1,\n    arrow = arrow(length = unit(0.05, \"npc\")),\n    aes(\n      x = lon_start, xend = lon_end,\n      y = lat_start, yend = lat_end\n    )\n  ) +\n  geom_text(\n    data = start,\n    color = .linecolor,\n    size = 4,\n    aes(x = lon, y = lat+0.0022, label = label_)\n  ) +\n  \n  # Best Split\n  geom_segment(\n    data = best_split_coord,\n    color = .linecolor,\n    linewidth = 1,\n    aes(x = lon, y = lat, yend = lat-0.012)\n  ) +\n  ggtext::geom_richtext(\n    data = best_split_coord,\n    nudge_y = -0.012,\n    color = 'white',\n    size = 4,\n    fill = .linecolor,\n    aes(x = lon, y = lat, label = label_)\n  ) +\n  \n  # Theme\n  theme_void(\n    base_family = 'Saira'\n  ) +\n  labs(\n    title = \"25 km @Caracas\",\n    tag = glue::glue(\"**Pace:** {round(metrics$pace, 2)}&lt;br&gt;**Elevation Gain:** {metrics$total_elevation_gain}\"),\n    caption = \"Source: Strava | Graphic: Rodrigo Serrano\"\n  ) +\n  theme(\n    plot.title = element_text(size = 25, hjust = .1, face = 'bold'),\n    plot.caption = element_text(hjust = .95),\n    plot.tag.position = c(0.80, 0.9),\n    plot.tag = ggtext::element_markdown(size = 10, hjust = 0)\n  )"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html#get-strava-activity-data",
    "href": "posts/strava-route-visualization/index.html#get-strava-activity-data",
    "title": "Visualize Running Routes",
    "section": "GET Strava Activity Data",
    "text": "GET Strava Activity Data\nTo extract data from strava, you need to create and register an app to get credentials. Follow steps in Getting Started.\nStrava API use OAuth authentication protocol. Hadley has an article that explain how to connect OAuth from R using httr2\nBasically it consist in getting credentials (client ID and secrets) and create the client to perform the authentication flow.\n\n# Set client\nclient &lt;- httr2::oauth_client(\n  id = Sys.getenv('STRAVA_CLIENT_ID'),\n  secret = Sys.getenv('STRAVA_CLIENT_SECRET'),\n  token_url = \"https://www.strava.com/oauth/token\",\n  name = \"strava-oauth-test\"\n)\n\n# Apply client flow\noauth &lt;- httr2::oauth_flow_auth_code(\n  client = client,\n  auth_url = 'https://www.strava.com/oauth/authorize',\n  scope = 'activity:read'\n)\n\n# Save temporal access token in environtment\nSys.setenv(\"STRAVA_ACCESS_TOKEN\" = oauth$access_token)\n\nActivity ID is explicit in the url of the strava webpage for the required activtity. We hit the /activity endpoint setting the activity target id\n\nactivity &lt;- request(\"https://www.strava.com/api/v3/activities/11886025399\") %&gt;% \n  httr2::req_auth_bearer_token(\n    Sys.getenv(\"STRAVA_ACCESS_TOKEN\")\n  ) %&gt;%\n  httr2::req_url_query(include_all_efforts = T) %&gt;% # Include Splits and efforts\n  req_perform() %&gt;% \n  resp_body_json()"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html#bulding-dataframes-for-the-visualization",
    "href": "posts/strava-route-visualization/index.html#bulding-dataframes-for-the-visualization",
    "title": "Visualize Running Routes",
    "section": "Bulding dataframes for the visualization",
    "text": "Bulding dataframes for the visualization\nWe got metrics for the activity like total distance, moving time and elevation. Lets store them to use in the visual\n\nmetrics &lt;- activity %&gt;% \n  keep_at(c('distance', 'moving_time', 'total_elevation_gain', 'average_heartrate')) %&gt;% \n  flatten_dfc() %&gt;% \n  mutate(\n    moving_time = moving_time/60,\n    distance = distance/1000,\n    pace = moving_time/distance\n  )\n\nmetrics\n\n# A tibble: 1 × 5\n  distance moving_time total_elevation_gain average_heartrate  pace\n     &lt;dbl&gt;       &lt;dbl&gt;                &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1     25.3        155.                 75.2              147.  6.11\n\n\nWe want to plot the route of the run, so we need to extract the polyline. This comes encoded with google algorithm, we can use googlePolylines package to decode it.\n\nroute &lt;- activity %&gt;% \n  pluck('map', 'polyline', 1) %&gt;% \n  googlePolylines::decode() %&gt;% \n  pluck(1) %&gt;% \n  mutate(\n    # Compute Distance between points to locate the fastest split in the polyline\n    dist = geosphere::distHaversine(\n      cbind(lon, lat),\n      cbind(lag(lon), lag(lat))\n    ),\n    distcum = cumsum(tidyr::replace_na(dist, 0))\n  )\n\nTo apply a map backgroung we can use ggmap and the free tier of stadia maps. It needs an API key that you could get folling instructions in the ggmap readme\n\nbbox &lt;- make_bbox(lon, lat, data = route)\nmap &lt;- get_stadiamap( bbox = bbox, maptype = \"stamen_toner_lite\", zoom = 14 )\n\nSo far we have elements to build a visual with the route in a city map background\n\n.linecolor &lt;- '#fc4c02'\n\nmap_route &lt;- ggmap(map) +\n\n  # Path (route)\n  geom_path(\n    data = route,\n    color = .linecolor,\n    linewidth = 2.5,\n    aes(x = lon, y = lat)\n  ) +\n  geom_path(\n    data = route,\n    color = 'white',\n    linewidth = 0.2,\n    aes(x = lon, y = lat)\n  )\n\nmap_route\n\n\n\n\n\n\n\n\nThere is no clear initial and finish points in the route. That is because I did a circuit. Lets add it to make it more explicit\n\nstart &lt;- route %&gt;%\n  # First record of route is start\n  slice_head(n = 1) %&gt;% \n  mutate(label_ = 'Start')\n\n  # Points to build an arrow to signal start direction\nstart_arrow &lt;- route %&gt;%\n  filter(\n    row_number() %in% c(1, 6)\n  ) %&gt;% \n  select(lat, lon) %&gt;% \n  mutate(\n    lat = lat+0.0012,\n    aux = c('start', 'end')\n  ) %&gt;% \n  tidyr::pivot_wider(names_from = aux, values_from = c(lat, lon))\n\nStrava also logs performance metrics for every kilometer, they call it splits. We can investigate the location of the fastest split.\n\nsplits &lt;- activity$splits_metric %&gt;% \n  map(~flatten(.x) %&gt;% as_tibble) %&gt;% \n  list_rbind() %&gt;% \n  mutate(\n    # Get pace in minutes per kilometer\n    pace = (lubridate::today() + lubridate::seconds_to_period(moving_time)) %&gt;% \n      format(format = '%M:%S') %&gt;% \n      stringr::str_remove('^0')\n  )\n\nbest_split &lt;- splits %&gt;% \n  filter(distance == 1000) %&gt;% # Omit final split with less than 1 km of distance\n  slice_min(order_by = moving_time, n = 1, with_ties = FALSE)\n\n# Last point of the fastest kilometer\nbest_split_coord &lt;- route %&gt;% \n  mutate(\n    closest_coord = abs(distcum-(best_split$split*1000))\n  ) %&gt;% \n  slice_min(order_by = closest_coord, n = 1, with_ties = FALSE) %&gt;% \n  mutate(\n    pace = best_split$pace,\n    avg_heartrate = best_split$average_heartrate,\n    label_ = glue::glue('&lt;span style=\"font-size:7pt;\"&gt;*end of*&lt;/span&gt; **Fastest Kilometer**&lt;br&gt;Pace: {pace}&lt;br&gt;Avg HeartRate: {round(avg_heartrate, 0)} bpm')\n  )"
  },
  {
    "objectID": "posts/strava-route-visualization/index.html#final-plot",
    "href": "posts/strava-route-visualization/index.html#final-plot",
    "title": "Visualize Running Routes",
    "section": "Final Plot",
    "text": "Final Plot\nLets include start point and fastest split in the visual. Also lets enhance fonts, this is a good post on how to use custom fonts in ggplot\n\n.linecolor &lt;- '#fc4c02'\n\nmap_route +\n  \n  # Start-Finish points\n  geom_point(\n    data = start,\n    size = 4, color = .linecolor,\n    aes(x = lon, y = lat)\n  ) +\n  \n  # Start-Finish Arrow Labels\n  geom_segment(\n    data = start_arrow,\n    color = .linecolor,\n    linewidth = 1,\n    arrow = arrow(length = unit(0.05, \"npc\")),\n    aes(\n      x = lon_start, xend = lon_end,\n      y = lat_start, yend = lat_end\n    )\n  ) +\n  geom_text(\n    data = start,\n    color = .linecolor,\n    size = 4,\n    aes(x = lon, y = lat+0.0022, label = label_)\n  ) +\n  \n  # Best Split\n  geom_segment(\n    data = best_split_coord,\n    color = .linecolor,\n    linewidth = 1,\n    aes(x = lon, y = lat, yend = lat-0.012)\n  ) +\n  ggtext::geom_richtext(\n    data = best_split_coord,\n    nudge_y = -0.012,\n    color = 'white',\n    size = 4,\n    fill = .linecolor,\n    aes(x = lon, y = lat, label = label_)\n  ) +\n  \n  # Theme\n  theme_void(\n    base_family = 'Saira'\n  ) +\n  labs(\n    title = \"25 km @Caracas\",\n    tag = glue::glue(\"**Pace:** {round(metrics$pace, 2)}&lt;br&gt;**Elevation Gain:** {metrics$total_elevation_gain}\"),\n    caption = \"Source: Strava | Graphic: Rodrigo Serrano\"\n  ) +\n  theme(\n    plot.title = element_text(size = 25, hjust = .1, face = 'bold'),\n    plot.caption = element_text(hjust = .95),\n    plot.tag.position = c(0.80, 0.92),\n    plot.tag = ggtext::element_markdown(size = 10, hjust = 0)\n  )"
  }
]